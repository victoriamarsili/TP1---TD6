---
title: ''
output:
  pdf_document: default
  html_document:
    df_print: paged
editor_options:
  markdown:
    wrap: sentence
---

\
Universidad Torcuato Di Tella

Licenciatura en Tecnología Digital\
**Tecnología Digital VI: Inteligencia Artificial**

# **Introduccion al problema**

Decidimos utilizar un dataset de aproximadamente cien mil canciones de spotify con el objetivo de predecir si una cancion es popular o no en funcion del resto de las variables. Los datos fueron extraidos desde la pagina de Kaggle.

Entre las variables principales podemos mencionar:

####Variable a predecir
- Popularity: Mide la popularidad de la cancion con un valor que va de 0 a 100

###Clasificacion Binaria
Decidimos abordar la clasificacion binaria estableciendo un umbral para definir si una cancion es o no es popular en funcion de la prediccion de la variable *popularity*


###Variables Predictoras
- duration_ms: La duración de la pista en milisegundos.
- explicit: Booleano que indica si la pista contiene contenido explícito.
- danceability: Describe cuán adecuada es una pista para bailar (0.0 = menos bailable, 1.0 = más bailable).
- energy: Representa la intensidad y actividad de una pista (0.0 = baja energía, 1.0 = alta energía).
- key: La clave musical de la pista mapeada usando la notación estándar de Clase de Tono.
- loudness: Volumen general de la pista en decibelios (dB).
- mode: Indica la modalidad (mayor o menor) de la pista.
- speechiness: Detecta la presencia de palabras habladas en la pista.
- acousticness: Medida de confianza de si la pista es acústica (0.0 = no acústica, 1.0 = altamente acústica).
- instrumentalness: Predice si una pista contiene voces (0.0 = contiene voces, 1.0 = instrumental).
- liveness: Detecta la presencia de una audiencia en la grabación (0.0 = grabación de estudio, 1.0 = actuación en vivo).
- valence: Mide la positividad musical transmitida por una pista (0.0 = negativa, 1.0 = positiva).
- tempo: Tempo estimado de la pista en pulsaciones por minuto (BPM).
- time_signature: Compás estimado de la pista (3 a 7).

Aclaracion: la variable popularity esta medida a partir de las reproducciones de la cancion.
A nivel de aplicacion, a un artista le seria util poder predecir la popularidad de una nueva cancion en funcion de las variables previamente mencionadas, ya que son variables que el artista puede controlar y modificar.


FALTA JUSTIFICACION ARBOL DE DECISION

### **Preparacion de los Datos**
Lo primero que hacemos es cargar el dataset de canciones y verificar si hay valores NA en el mismo.

```{r}
library(readr)
spotify_data  = read.csv("spotify_data.csv")

table(is.na(spotify_data))
```
Vemos que no hay ningun dato faltante.

Una vez cargado, empieza la etapa de limpieza de los datos. Por ejemplo, lo primero que notamos fue que el dataset contenia muchas repeticiones de canciones por lo que eliminamos dichas observaciones repetidas, disminuyendo la cantidad total de canciones de 114000 a 73609. Posterior a eso, procedemos a eliminar las variables "track_id", "artists" "album_name" y "track_name" dado que no son relevantes para el trabajo practico.

```{r}

#CODIGO DE PYTHON

```


```{r}
# cargamos el nuevo dataset eliminando las variables anteriormente mencionadas
datos <- read_csv("cleaned_dataset.csv",col_select =-c(track_id,artists,album_name,track_name), show_col_types = FALSE)
```

Luego, hacemos un sampleo aleatorio de 50000 observaciones ya que, como dijimos previamente, nuestro dataset resultante tiene 73609 observaciones (el enunciado indica que el mismo no debe superar las 50000 observaciones)


```{r}
set.seed(33016244)

if (nrow(datos) >= 50000) {
  # Crear un vector de índices aleatorios
  indices <- sample(1:nrow(datos), 50000)
  
  # Seleccionar las filas correspondientes a esos índices
  muestra <- datos[indices, ]
  
  # Verificar el tamaño de la muestra
  nrow(muestra)
}
```
Verificamos que, efectivamente, el dataset resultante tiene 50000 observaciones.


Por ultimo, convertimos las observaciones de la variable "duration_ms" (que indica la duracion de una cancion expresada en milisegundos) de milisegundos a minutos. Tambien renombramos la variable como "duration_min"

```{r}
# Reemplazar duration_ms con la duración en minutos
muestra$duration_ms <- muestra$duration_ms / 60000

# Renombrar la variable a duration_min para reflejar el nuevo contenido
names(muestra)[names(muestra) == "duration_ms"] <- "duration_min"
```

### **Estadistica Descriptiva**

```{r}

# Apply summary to each column in the dataset
sapply(muestra, summary)

```


```{r}
hist(muestra$popularity, main = "Histograma de popularidad", xlab = "popularidad", ylab = "frecuencia")
abline(v = mean(muestra$popularity), col = "orange", lwd = 9)
```


Vemos que la media de *popularity* es 34.30186, por lo que resulta razonable establecer el umbral de popularidad para la clasificacion binaria en un valor cercano a la media. Por otro lado, el desvio estandar es 'r sd(muestra$popularity)'. Creemos que mover el umbral un desvio a la derecha seria una medida extrema, que se alejaria bastante de la media. Por lo tanto, decidimos establecer el umbral de popularidad aproxmadamente 1/4 de desvio estandar a la derecha de la media, fijando el valor en 40.
Teniendo eso en cuenta, vamos a realizar una transformación de la variable *popularity*, donde todas las observaciones mayores o iguales a 40 seran cambiadas por un 1 (la cancion es popular) y, en el caso contrario, con un 0 (la cancion no es popular).

```{r}
#Convertir la popularidad a una variable numerica binaria
muestra$popularity <- ifelse(muestra$popularity >= 40, 1, 0)


# Convertir la columna a factor
muestra$popularity = as.factor(muestra$popularity)

table(is.na(muestra))
```


##Division del dataset en train, validation y test

```{r}
n <- nrow(muestra)
train_indices <- sample(1:n, size = 0.7 * n)
remaining_indices <- setdiff(1:n, train_indices)
val_indices <- sample(remaining_indices, size = 0.15 * n)
test_indices <- setdiff(remaining_indices, val_indices)
train_set <- muestra[train_indices, ]
val_set <- muestra[val_indices, ]
test_set <- muestra[test_indices, ]

```


REMINDER: OUTLIERS DE muestra$duration_min


REMINDER: CONVERSION DE LA VARIABLE track_genre



### **Árbol de decisión**

```{r}
library(rpart)

arbol = rpart(formula = popularity ~ duration_min + explicit + danceability + energy + key + loudness + mode + speechiness + acousticness + instrumentalness + liveness + valence + tempo + time_signature, data = train_set, method = "class")

library(rpart.plot)
rpart.plot(arbol, box.palette = "orange")

print(rpart.control())
```

###Analisis de la estructura del arbol
En el nodo raiz, podemos observar que la probabilidad de que una cancion sea popular (en esta instancia) es del 41% aproximadamente, lo cual tiene sentido ya que en esta instancia el modelo no conoce nada sobre la cancion por lo tanto esta probabilidad deberia coincidir con la media de popularity para el train_set.

```{r}
mean(train_set$popularity)
```
REMINDER: HABLAR SOBRE LA DIFERENCIA DE MEANS NUMERICA VS CATEGORICA

En adicion a esto, el modelo toma a la variable *instrumentalness* como primer filtro, estableciendo un umbral de 0.013. En este caso, el 33% del conjunto de entrenamiento cumple con esta condicion, para las cuales el arbol predice que ninguna es popular. Con el 67% restante, ahora se pregunta si *speechiness* es mayor o igual a 0.56. Podemos observar que unicamente el 1% de los datos cumple con esto. El arbol realiza este proceso de filtrado con diferentes variables hasta que el conjunto de datos no supera el *min_split* que, como vimos, esta establecido en 20 obervaciones por defecto.

REMINDER: PREGUNTAR COMO HACER ESTE ANALISIS DE MANERA IDONEA

###Análisis de los hiperparámetros

Los hiperparámetros más relevantes y su valor por defecto en rpart son:

- minsplit = 20 --> un nodo debe tener al menos 20 observaciones para ser considerado para una división, puede suceder como no.

- minbucket = round(minsplit / 3) en este caso 20/ 3 = 6.66 ≈ 7--> por defecto, es aproximadamente un tercio de minsplit y limita el número mínimo de observaciones que debe tener un nodo terminal.

- (complexity parameter) cp = 0.01--> controla la poda del árbol, en donde un valor de 0.01 significa que una división debe disminuir el error relativo en al menos un 1% para ser considerada. Un valor de 1 se corresponde con un árbol sin divisiones, mientras que un valor de 0, con un árbol de profundidad máxima. Sólo se agregan divisiones cuando el costo de agregarlas es menor al valor de `cp`.

- maxdepth = 30 --> un valor de 30 permite que el árbol tenga hasta 30 niveles de profundidad, este hiperparámetro controla la profundiad del mismo. 

- xval = 10 --> una significación de 10 implica que se realizarán 10 validaciones cruzadas.

- maxcompete = 4 --> una estimación de 4 implica que retendrán las 4 mejores divisiones alternativas en cada nodo.

- maxsurrogate = 5 --> define el número máximo de variables sustitutas a retener en cada nodo. Las variables sustitutas se utilizan para manejar datos faltantes. Un valor de 5 significa que el algoritmo retendrá hasta 5 variables sustitutas por nodo.

- usesurrogate = 2 --> controla cómo se utilizan las variables sustitutas para manejar datos faltantes. Los valores posibles son:

  0 = No se utilizan variables sustitutas.
  1 = Se utilizan variables sustitutas solo para dividir los datos.
  2 = Se utilizan variables sustitutas tanto para dividir los datos como para asignar observaciones a nodos         terminales.

- surrogatestyle = 0 --> define el estilo de selección de variables sustitutas. Los valores posibles son:
  
  0 = Se seleccionan las variables sustitutas basándose en la reducción del error.
  1 = Se seleccionan las variables sustitutas basándose en la similitud con la variable principal.


###Predicciones

```{r}
predictions_class  <- predict(arbol, newdata = test_set, type = "class")

test_set$predicted_class_popularity <- predictions_class

predictions_prob  <- predict(arbol, newdata = test_set, type = "prob")

test_set$predicted_prob_popularity <- predictions_prob
```


###Metricas de Performance

Matriz de confusión

```{r}
library(caret)
library(recipes)

#confusion_matrix <- ConfusionMatrix(test_set$predicted_class_popularity, test_set$popularity)
confusion_matrix <- confusionMatrix(test_set$predicted_class_popularity, test_set$popularity)


cm <- confusion_matrix$table

positive_len <- length(test_set$popularity[test_set$popularity == 1])
negative_len <- length(test_set$popularity[test_set$popularity == 0])

cm[1, 1] <- cm[1, 1] / negative_len
cm[1, 2] <- cm[1, 2] / positive_len
cm[2, 1] <- cm[2, 1] / negative_len
cm[2, 2] <- cm[2, 2] / positive_len

cm
```
```{r}
library(pheatmap)
# Crear el mapa de calor
pheatmap(cm,
         display_numbers = TRUE,
         color = colorRampPalette(c("coral", "gold"))(50),
         main = "Confusion Matrix",
         number_format = "%.2f",
         cluster_rows = FALSE,
         cluster_cols = FALSE,
         legend = TRUE)
```

Accuracy

```{r}
true_positive = cm[2, 2] * positive_len
true_positive

false_positive = cm[2, 1] * positive_len
false_positive

false_negative = cm[1, 2] * negative_len
false_negative

true_negative = cm[1, 1] * negative_len
true_negative

accuracy = (true_positive + true_negative)/(positive_len + negative_len)
accuracy
```

Precision y recall

```{r}
precision = true_positive/(true_positive + false_positive)
precision

recall = true_positive/(true_positive + false_negative)
recall
```

F1 - score

```{r}
f1_score = (2*precision*recall)/(precision + recall)
f1_score
```

AUC-ROC

```{r}
library(pROC)
vector_predicciones = as.numeric(test_set$predicted_class_popularity)
curva_roc = roc(test_set$popularity, vector_predicciones)

# Extraer el área bajo la curva (AUC)
auc_value <- auc(curva_roc)

# Imprimir solo el área bajo la curva (AUC)
print(auc_value)

plot(curva_roc, main = "Curva de AUC-ROC", col = "orange", lwd = 3)
```

###Interpretacion de las metricas de performance

Matriz de confusion:

  - TP: 0.3097139 -> Proporción de verdaderos positivos en relación con el total de verdaderos positivos.
  - TN: 0.8340009 -> Proporción de verdaderos negativos en relación con el total de verdaderos negativos.
  - FP: 0.1659991 -> Proporción de falsos positivos en relación con el total de verdaderos negativos.
  - FN: 0.6902861 -> Proporción de falsos negativos en relación con el total de verdaderos positivos.

Accuracy: El modelo tiene una tasa de precisión general del 62.39%, lo que indica un rendimiento moderado.

Precision: Un valor de 0.555158 sugiere que el 55.52% de las veces que el modelo predice la clase positiva, esta predicción es correcta.

Recall: Un valor de 0.3097139 indica que el modelo ha identificado correctamente el 30.97% de los verdaderos positivos. Esto sugiere que el modelo es menos eficaz en identificar la clase positiva.

F1-Score: Un valor de 0.3976084 indica el equilibrio entre la precisión y el recall. Es una medida general del rendimiento del modelo en términos de precisión y exhaustividad.

area bajo la curva de AUC-ROC: Un AUC de 0.5719 indica que el modelo tiene una capacidad discriminativa ligeramente mejor que la aleatoria, pero no es excepcional. Esto sugiere que el modelo tiene un rendimiento moderado en la discriminación entre clases.



5. Optimización del modelo

 -> Modificar valores de los hiperparámetros maxdepth, minsplit y minbucket, y evaluar el rendimiento de cada arból con el conjunto de validación. 
 IMPORTANTE! SOLO USANDO AUC-ROC Y DEJAR FIJO CP Y XVAL EN 0
 
```{r}
library(rBayesianOptimization)
library(rpart)
library(pROC)
library(ggplot2)

# Función para evaluar el rendimiento del árbol
evaluar_arbol <- function(maxdepth, minsplit, minbucket) {
  modelo <- rpart(
    formula = train_set$popularity ~ ., 
    data = train_set, 
    method = "class", 
    control = rpart.control(
      maxdepth = as.integer(maxdepth), 
      minsplit = as.integer(minsplit), 
      minbucket = as.integer(minbucket),
      cp = 0,
      xval = 0
    )
  )
  predicciones <- predict(modelo, val_set, type = "prob")[,2]
  roc_curve <- roc(val_set$popularity, predicciones)
  auc_score <- auc(roc_curve)
  
  # Devolver una lista con el campo Score
  list(Score = auc_score)
}

# Definir los límites de los hiperparámetros
limites <- list(
  maxdepth = c(7L, 20L), 
  minsplit = c(5L, 30L), 
  minbucket = c(1L, L)
)

# Ejecutar la optimización bayesiana
result <- BayesianOptimization(
  FUN = evaluar_arbol,
  bounds = limites,
  init_points = 5,
  n_iter = 95
)

# Extraer el historial de resultados
result_data <- result$History

ggplot(result_data, aes(x = Round, y = Value)) +
  geom_line(color = "blue") +
  geom_point(color = "red") +
  geom_hline(yintercept = 0.8844099, color = "orange", linetype = "solid", size = 1, alpha = 0.70) +
  labs(title = "Progreso del valor del area bajo la curva de AUC-ROC con Optimización Bayesiana",
       x = "Iteración",
       y = "Área Bajo la Curva de AUC-ROC") +
  theme_minimal()
```
 En este grafico, podemos observar que realizar 100 iteraciones no es necesario ya que aproximadamente a las 25 iteraciones se puede observar una convergencia al maximo valor del area bajo la curva de AUC-ROC que es ≈ 0.8844. Este trade-off entre tiempo de ejecucion y performance del modelo es clave a la hora de establecer el valor de *n_iter*.
 
 
 
###Visualizaciones relacion performance - hiperparametro
 
```{r}
library(plotly)

# Crear gráfico 3D
fig <- plot_ly(
  data = result_data,
  x = ~maxdepth,
  y = ~minsplit,
  z = ~Value,
  type = 'scatter3d',
  mode = 'markers',
  marker = list(size = 5, color = ~Value, colorscale = 'Viridis', showscale = TRUE)
)

# Configurar el título y los ejes
fig <- fig %>%
  layout(
    title = 'Progreso del AUC en función de maxdepth y minsplit',
    scene = list(
      xaxis = list(title = 'maxdepth'),
      yaxis = list(title = 'minsplit'),
      zaxis = list(title = 'Área Bajo la Curva (AUC)')
    )
  )

# Mostrar el gráfico
fig
```
 
```{r}
#GRAFICO 2: performance en funcion de minsplit


# Extraer el historial de resultados
ggplot(result_data, aes(x = minsplit, y = Value)) +
  geom_line(color = "blue") +
  geom_point(color = "red") +
  geom_hline(yintercept = 0.8844099, color = "orange", linetype = "solid", size = 1, alpha = 0.70) +
  labs(title = "performance en funcion de minsplit con n = 100",
       x = "minsplit",
       y = "Área Bajo la Curva de AUC-ROC") +
  theme_minimal()
```


 -> Elegir el mejor árbol y medir el valor de AUC-ROC con el conjunto de testeo
 
 [ACA]
 
 -> Comparar el rendimiento entre el primer árbol y el árbol optimizado
 
 [ACA]
 
 6. Intepretación de resultados
 
 -> Presentar el árbol final, y detallar las diferencias entre el primero y el último
 
 [ACA]
 
 -> Identificar y discutir las variables más importantes según nuestro mejor árbol
 
 [ACA]
 
 7. Análisis del impacto de los valores faltantes
 
 -> Tenemos que generar 3 datasets, en cada uno falta un % de los datos de las predictoras.     En el primero 20%, el segundo 50% y el tercero 75%
 
 [ACA]
 
 -> Para cada dataset creamos un nuevo árbol optimizado (elegir hiperparam. que maximicen)
    Comparar el mejor árbol obtenido antes, con los 3 nuevos árboles
    
  [ACA]
    
  -> Analizar como cambia el rendimiento del modelo a medida que aumenta la cantidad de NAs (Recomiendan fuertemente usar gráficos)
  
  [ACA]

8. Conclusiones y discusión

  -> Resumir los hallazgos principales del análisis.
  
  [ACA]
  
  -> Reflexionar sobre la efectividad del árbol de decisión para el problema planteado.
  
  [ACA]
  
  -> Sugerir posibles mejoras o direcciones futuras para el análisis.
  
  [ACA]

```{r}
train_withoutNA <- train                                             
train_withoutNA$Age[is.na(train_withoutNA$Age)] <- mean(train_withoutNA$Age, na.rm = TRUE)

tree_withoutNA <- rpart(formula = Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
                        data = train_withoutNA,
                        method = "class")

predictions_withoutNA <- predict(tree_withoutNA, newdata = test, type = "class")

data.frame("predicciones_con_na" = predictions,
           "predicciones_sin_na" = predictions_withoutNA)
```

Podemos ver que, para algunas observaciones, la predicción es diferente (e.g., fila 19).
Entonces, nos puede surgir la siguiente pregunta: ¿es mejor imputar los datos faltantes o dejar que el árbol por sí solo se encargue de ellos?

Esto y *mucho más* lo veremos en el TP1.
