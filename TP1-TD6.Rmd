---
title: ''
output:
  pdf_document: default
  html_document:
    df_print: paged
editor_options:
  markdown:
    wrap: sentence
---

\
Universidad Torcuato Di Tella

Licenciatura en Tecnología Digital\
**Tecnología Digital VI: Inteligencia Artificial**

# **Introduccion al problema**

Decidimos utilizar un dataset de aproximadamente cien mil canciones de spotify con el objetivo de predecir si una cancion es popular o no en funcion del resto de las variables. Los datos fueron extraidos desde la pagina de Kaggle.

Entre las variables principales podemos mencionar:

####Variable a predecir
- Popularity: Mide la popularidad de la cancion con un valor que va de 0 a 100

###Clasificacion Binaria
Decidimos abordar la clasificacion binaria estableciendo un umbral para definir si una cancion es o no es popular en funcion de la prediccion de la variable *popularity*


###Variables Predictoras
- duration_ms: La duración de la pista en milisegundos.
- explicit: Booleano que indica si la pista contiene contenido explícito.
- danceability: Describe cuán adecuada es una pista para bailar (0.0 = menos bailable, 1.0 = más bailable).
- energy: Representa la intensidad y actividad de una pista (0.0 = baja energía, 1.0 = alta energía).
- key: La clave musical de la pista mapeada usando la notación estándar de Clase de Tono.
- loudness: Volumen general de la pista en decibelios (dB).
- mode: Indica la modalidad (mayor o menor) de la pista.
- speechiness: Detecta la presencia de palabras habladas en la pista.
- acousticness: Medida de confianza de si la pista es acústica (0.0 = no acústica, 1.0 = altamente acústica).
- instrumentalness: Predice si una pista contiene voces (0.0 = contiene voces, 1.0 = instrumental).
- liveness: Detecta la presencia de una audiencia en la grabación (0.0 = grabación de estudio, 1.0 = actuación en vivo).
- valence: Mide la positividad musical transmitida por una pista (0.0 = negativa, 1.0 = positiva).
- tempo: Tempo estimado de la pista en pulsaciones por minuto (BPM).
- time_signature: Compás estimado de la pista (3 a 7).

Aclaracion: la variable popularity esta medida a partir de las reproducciones de la cancion.
A nivel de aplicacion, a un artista le seria util poder predecir la popularidad de una nueva cancion en funcion de las variables previamente mencionadas, ya que son variables que el artista puede controlar y modificar.


FALTA JUSTIFICACION ARBOL DE DECISION

### **Preparacion de los Datos**
Lo primero que hacemos es cargar el dataset de canciones y verificar si hay valores NA en el mismo.

```{r}
library(readr)
spotify_data  = read.csv("spotify_data.csv")
spotify_data <- subset(spotify_data, select = -track_genre)
spotify_data <- subset(spotify_data, select = -X)

table(is.na(spotify_data))
```
Vemos que no hay ningun dato faltante.

Una vez cargado, empieza la etapa de limpieza de los datos. Por ejemplo, lo primero que notamos fue que el dataset contenia muchas repeticiones de canciones por lo que eliminamos dichas observaciones repetidas, disminuyendo la cantidad total de canciones de 114000 a 73609. Posterior a eso, procedemos a eliminar las variables "track_id", "artists" "album_name" y "track_name" dado que no son relevantes para el trabajo practico.

```{r}

#CODIGO DE PYTHON

```


```{r}
# cargamos el nuevo dataset eliminando las variables anteriormente mencionadas
datos <- read_csv("cleaned_dataset.csv",col_select =-c(track_id,artists,album_name,track_name), show_col_types = FALSE)
datos <- subset(datos, select = -track_genre)

# Eliminar la primera columna usando subset
datos <- subset(datos, select = -1)

```

Luego, hacemos un sampleo aleatorio de 50000 observaciones ya que, como dijimos previamente, nuestro dataset resultante tiene 73609 observaciones (el enunciado indica que el mismo no debe superar las 50000 observaciones)


```{r}
set.seed(33016244)

if (nrow(datos) >= 50000) {
  # Crear un vector de índices aleatorios
  indices <- sample(1:nrow(datos), 50000)
  
  # Seleccionar las filas correspondientes a esos índices
  muestra <- datos[indices, ]
  
  # Verificar el tamaño de la muestra
  nrow(muestra)
}

```
Verificamos que, efectivamente, el dataset resultante tiene 50000 observaciones.


Por ultimo, convertimos las observaciones de la variable "duration_ms" (que indica la duracion de una cancion expresada en milisegundos) de milisegundos a minutos. Tambien renombramos la variable como "duration_min"

```{r}
# Reemplazar duration_ms con la duración en minutos
muestra$duration_ms <- muestra$duration_ms / 60000

# Renombrar la variable a duration_min para reflejar el nuevo contenido
names(muestra)[names(muestra) == "duration_ms"] <- "duration_min"
```

### **Estadistica Descriptiva**

```{r}

# Apply summary to each column in the dataset
sapply(muestra, summary)

```


```{r}
hist(muestra$popularity, main = "Histograma de popularidad", xlab = "popularidad", ylab = "frecuencia")
abline(v = mean(muestra$popularity), col = "orange", lwd = 9)
```


Vemos que la media de *popularity* es 34.30186, por lo que resulta razonable establecer el umbral de popularidad para la clasificacion binaria en un valor cercano a la media. Por otro lado, el desvio estandar es 'r sd(muestra$popularity)'. Creemos que mover el umbral un desvio a la derecha seria una medida extrema, que se alejaria bastante de la media. Por lo tanto, decidimos establecer el umbral de popularidad aproxmadamente 1/4 de desvio estandar a la derecha de la media, fijando el valor en 40.
Teniendo eso en cuenta, vamos a realizar una transformación de la variable *popularity*, donde todas las observaciones mayores o iguales a 40 seran cambiadas por un 1 (la cancion es popular) y, en el caso contrario, con un 0 (la cancion no es popular).

```{r}
#Convertir la popularidad a una variable numerica binaria
muestra$popularity <- ifelse(muestra$popularity >= 34, 1, 0)


# Convertir la columna a factor
muestra$popularity = as.factor(muestra$popularity)


muestra$explicit <- ifelse(muestra$explicit == TRUE, 1, 0)
```


##Division del dataset en train, validation y test

```{r}
n <- nrow(muestra)

train_indices <- sample(1:n, size = 0.7 * n)
remaining_indices <- setdiff(1:n, train_indices)
val_indices <- sample(remaining_indices, size = 0.15 * n)
test_indices <- setdiff(remaining_indices, val_indices)

train_set <- muestra[train_indices, ]
val_set <- muestra[val_indices, ]
test_set <- muestra[test_indices, ]
test_set_arbol = muestra[test_indices, ]
```


REMINDER: OUTLIERS DE muestra$duration_min


REMINDER: CONVERSION DE LA VARIABLE track_genre

### **Árbol de decisión**

```{r}
library(rpart)
library(rpart.plot)

arbol = rpart(formula = popularity ~ ., data = train_set, method = "class")

rpart.plot(arbol, box.palette = "orange")

print(rpart.control())
```


###Analisis de la estructura del arbol
En el nodo raiz, podemos observar que la probabilidad de que una cancion sea popular (en esta instancia) es del 41% aproximadamente, lo cual tiene sentido ya que en esta instancia el modelo no conoce nada sobre la cancion por lo tanto esta probabilidad deberia coincidir con la media de popularity para el train_set.

En adicion a esto, el modelo toma a la variable *instrumentalness* como primer filtro, estableciendo un umbral de 0.013. En este caso, el 33% del conjunto de entrenamiento cumple con esta condicion, para las cuales el arbol predice que ninguna es popular. Con el 67% restante, ahora se pregunta si *speechiness* es mayor o igual a 0.56. Podemos observar que unicamente el 1% de los datos cumple con esto. El arbol realiza este proceso de filtrado con diferentes variables hasta que el conjunto de datos no supera el *min_split* que, como vimos, esta establecido en 20 obervaciones por defecto.

REMINDER: PREGUNTAR COMO HACER ESTE ANALISIS DE MANERA IDONEA

###Análisis de los hiperparámetros

Los hiperparámetros más relevantes y su valor por defecto en rpart son:

- minsplit = 20 --> un nodo debe tener al menos 20 observaciones para ser considerado para una división, puede suceder como no.

- minbucket = round(minsplit / 3) en este caso 20/ 3 = 6.66 ≈ 7--> por defecto, es aproximadamente un tercio de minsplit y limita el número mínimo de observaciones que debe tener un nodo terminal.

- (complexity parameter) cp = 0.01--> controla la poda del árbol, en donde un valor de 0.01 significa que una división debe disminuir el error relativo en al menos un 1% para ser considerada. Un valor de 1 se corresponde con un árbol sin divisiones, mientras que un valor de 0, con un árbol de profundidad máxima. Sólo se agregan divisiones cuando el costo de agregarlas es menor al valor de `cp`.

- maxdepth = 30 --> un valor de 30 permite que el árbol tenga hasta 30 niveles de profundidad, este hiperparámetro controla la profundiad del mismo. 

- xval = 10 --> una significación de 10 implica que se realizarán 10 validaciones cruzadas.

- maxcompete = 4 --> una estimación de 4 implica que retendrán las 4 mejores divisiones alternativas en cada nodo.

- maxsurrogate = 5 --> define el número máximo de variables sustitutas a retener en cada nodo. Las variables sustitutas se utilizan para manejar datos faltantes. Un valor de 5 significa que el algoritmo retendrá hasta 5 variables sustitutas por nodo.

- usesurrogate = 2 --> controla cómo se utilizan las variables sustitutas para manejar datos faltantes. Los valores posibles son:

  0 = No se utilizan variables sustitutas.
  1 = Se utilizan variables sustitutas solo para dividir los datos.
  2 = Se utilizan variables sustitutas tanto para dividir los datos como para asignar observaciones a nodos         terminales.

- surrogatestyle = 0 --> define el estilo de selección de variables sustitutas. Los valores posibles son:
  
  0 = Se seleccionan las variables sustitutas basándose en la reducción del error.
  1 = Se seleccionan las variables sustitutas basándose en la similitud con la variable principal.


###Predicciones

```{r}

predictions_class  <- predict(arbol, newdata = test_set, type = "class")

test_set$predicted_class_popularity <- predictions_class

agregar_predicciones = function(dataset, nombre_columna_predicciones, arbol) {
  predictions_class  <- predict(arbol, newdata = dataset, type = "class")

  dataset[[nombre_columna_predicciones]] <- predictions_class
  
  message("Columna ", nombre_columna_predicciones, " agregada exitosamente al dataset.")
  #predictions_prob  <- predict(arbol, newdata = test_set, type = "prob")
  
  #test_set$predicted_prob_popularity <- predictions_prob
}

agregar_predicciones(test_set, "predicted_class_popularity", arbol)
```


###Metricas de Performance

Matriz de confusión

```{r}
library(caret)
library(recipes)

positive_len <- length(test_set$predicted_class_popularity[test_set$predicted_class_popularity == 1])
negative_len <- length(test_set$predicted_class_popularity[test_set$predicted_class_popularity == 0])

generar_matriz_confusion = function(clase_real, clase_predicha){
  #Requiere: el formato de los parametros sea "dataset$clase"
  
  confusion_matrix = confusionMatrix(clase_predicha, clase_real)
  
  cm = confusion_matrix$table
  
  positive_len <- length(clase_real[clase_real == 1])
  negative_len <- length(clase_real[clase_real == 0])
  
  cm[1, 1] <- cm[1, 1] / negative_len
  cm[1, 2] <- cm[1, 2] / positive_len
  cm[2, 1] <- cm[2, 1] / negative_len
  cm[2, 2] <- cm[2, 2] / positive_len
  
  return(cm)
}

cm = generar_matriz_confusion(test_set$popularity, test_set$predicted_class_popularity)
```



```{r}
#Graficamos la matriz de confusion
library(pheatmap)

graficar_matriz_confusion = function(tabla_matriz_confusion) {
  # Crear el mapa de calor
  pheatmap(tabla_matriz_confusion,
           display_numbers = TRUE,
           color = colorRampPalette(c("coral", "gold"))(50),
           main = "Confusion Matrix",
           number_format = "%.2f",
           cluster_rows = FALSE,
           cluster_cols = FALSE,
           legend = TRUE,
           fontsize_number = 15)
}

graficar_matriz_confusion(cm)
```

Accuracy

```{r}

true_positive = cm[2, 2] * positive_len
  
false_positive = cm[2, 1] * positive_len

false_negative = cm[1, 2] * negative_len

true_negative = cm[1, 1] * negative_len

calcular_accuracy = function(positivos, negativos) {
  true_positive = cm[2, 2] * positivos
  true_positive
  
  false_positive = cm[2, 1] * positivos
  false_positive
  
  false_negative = cm[1, 2] * negativos
  false_negative
  
  true_negative = cm[1, 1] * negativos
  true_negative
  
  accuracy = (true_positive + true_negative)/(positivos + negativos)
  accuracy
  
  return(accuracy)
}

calcular_accuracy(positive_len, negative_len)
```

Precision y recall

```{r}
calcular_precision = function(TP, FP) {
  precision = TP/(TP + FP)
  print(sprintf("Precision: %f", precision))
}

calcular_recall = function(TP, FN) {
  recall = TP/(TP + FN)
  print(sprintf("Recall: %f", recall))
}

precision = calcular_precision(true_positive, false_positive)
recall = calcular_recall(true_positive, false_negative)
```

F1 - score

```{r}
calcular_f1_score = function(precision, recall) {
  f1_score = (2*precision*recall)/(precision + recall)
  print(sprintf("f1 score: %f", f1_score))
}

calcular_f1_score(precision, recall)
```

AUC-ROC

```{r}
library(pROC)

curva_AUC_ROC = function(clase_predicha, clase_real) {
  vector_predicciones = as.numeric(clase_predicha)
  curva_roc = roc(clase_real, vector_predicciones)
  
  # Extraer el área bajo la curva (AUC)
  auc_value <- auc(curva_roc)
  
  # Imprimir solo el área bajo la curva (AUC)
  print(sprintf("Area bajo la curva de AUC-ROC: %f", auc_value))
  
  plot(curva_roc, main = "Curva de AUC-ROC", col = "orange", lwd = 3)
  
  return(curva_roc)
}

curva_default = curva_AUC_ROC(test_set$predicted_class_popularity, test_set$popularity)
```

###Interpretacion de las metricas de performance

Matriz de confusion:

  - TP: 0.3097139 -> Proporción de verdaderos positivos en relación con el total de verdaderos positivos.
  - TN: 0.8340009 -> Proporción de verdaderos negativos en relación con el total de verdaderos negativos.
  - FP: 0.1659991 -> Proporción de falsos positivos en relación con el total de verdaderos negativos.
  - FN: 0.6902861 -> Proporción de falsos negativos en relación con el total de verdaderos positivos.

Accuracy: El modelo tiene una tasa de precisión general del 62.39%, lo que indica un rendimiento moderado.

Precision: Un valor de 0.555158 sugiere que el 55.52% de las veces que el modelo predice la clase positiva, esta predicción es correcta.

Recall: Un valor de 0.3097139 indica que el modelo ha identificado correctamente el 30.97% de los verdaderos positivos. Esto sugiere que el modelo es menos eficaz en identificar la clase positiva.

F1-Score: Un valor de 0.3976084 indica el equilibrio entre la precisión y el recall. Es una medida general del rendimiento del modelo en términos de precisión y exhaustividad.

area bajo la curva de AUC-ROC: Un AUC de 0.5719 indica que el modelo tiene una capacidad discriminativa ligeramente mejor que la aleatoria, pero no es excepcional. Esto sugiere que el modelo tiene un rendimiento moderado en la discriminación entre clases.



###5. Optimización del modelo

 -> Modificar valores de los hiperparámetros maxdepth, minsplit y minbucket, y evaluar el rendimiento de cada arból con el conjunto de validación. 
 IMPORTANTE! SOLO USANDO AUC-ROC Y DEJAR FIJO CP Y XVAL EN 0
 
```{r}
library(rBayesianOptimization)
library(rpart)
library(pROC)
library(ggplot2)

# Función para evaluar el rendimiento del árbol, con dataset como parámetro
evaluar_arbol <- function(maxdepth, minsplit, minbucket, train_set, val_set) {
  modelo <- rpart(
    formula = train_set$popularity ~ ., 
    data = train_set, 
    method = "class", 
    control = rpart.control(
      maxdepth = as.integer(maxdepth), 
      minsplit = as.integer(minsplit), 
      minbucket = as.integer(minbucket),
      cp = 0,
      xval = 0
    )
  )
  predicciones <- predict(modelo, val_set, type = "prob")[,2]
  roc_curve <- roc(test_set$popularity, predicciones)
  auc_score <- auc(roc_curve)
  
  # Devolver una lista con el campo Score
  list(Score = auc_score)
}

# Definir los límites de los hiperparámetros
limites <- list(
  maxdepth = c(5L, 30L), 
  minsplit = c(0L, 100L), 
  minbucket = c(1L, 10L)
)

# Ejecutar la optimización bayesiana
result <- BayesianOptimization(
  FUN = function(maxdepth, minsplit, minbucket) {
    evaluar_arbol(maxdepth, minsplit, minbucket, train_set, val_set)
  },
  bounds = limites,
  init_points = 25,
  n_iter = 25,
)

graficar_performance = function(resultado){
  ggplot(resultado, aes(x = Round, y = Value)) +
  geom_line(color = "blue") +
  geom_point(color = "red") +
  geom_hline(yintercept = 0.6562452, color = "orange", linetype = "solid", size = 1, alpha = 0.70) +
  labs(title = "Progreso del valor del area bajo la curva de AUC-ROC con Optimización Bayesiana",
       x = "Iteración",
       y = "Área Bajo la Curva de AUC-ROC") +
  theme_minimal()
}

# Extraer el historial de resultados
result_data <- result$History

mejores_hiperparametros = result$Best_Par

graficar_performance(result_data)
```
 En este grafico, podemos observar que realizar 100 iteraciones no es necesario ya que aproximadamente a las 25 iteraciones se puede observar una convergencia al maximo valor del area bajo la curva de AUC-ROC que es ≈ 0.8844. Este trade-off entre tiempo de ejecucion y performance del modelo es clave a la hora de establecer el valor de *n_iter*.
 
 
 
###Visualizaciones relacion performance - hiperparametro
 
```{r}
library(plotly)

# Crear gráfico 3D
fig <- plot_ly(
  data = result_data,
  x = ~maxdepth,
  y = ~minsplit,
  z = ~Value,
  type = 'scatter3d',
  mode = 'markers',
  marker = list(size = 5, color = ~Value, colorscale = 'Viridis', showscale = TRUE)
)

# Configurar el título y los ejes
fig <- fig %>%
  layout(
    title = 'Progreso del AUC en función de maxdepth y minsplit',
    scene = list(
      xaxis = list(title = 'maxdepth'),
      yaxis = list(title = 'minsplit'),
      zaxis = list(title = 'Área Bajo la Curva (AUC)')
    )
  )

# Mostrar el gráfico
fig
```
 
 
 
 
```{r}
library(plotly)

# Crear gráfico 3D
fig2 <- plot_ly(
  data = result_data,
  x = ~maxdepth,
  y = ~minbucket,
  z = ~Value,
  type = 'scatter3d',
  mode = 'markers',
  marker = list(size = 5, color = ~Value, colorscale = 'Viridis', showscale = TRUE)
)

# Configurar el título y los ejes
fig2 <- fig2 %>%
  layout(
    title = 'Progreso del AUC en función de maxdepth y minbucket',
    scene = list(
      xaxis = list(title = 'maxdepth'),
      yaxis = list(title = 'minbucket'),
      zaxis = list(title = 'Área Bajo la Curva (AUC)')
    )
  )

# Mostrar el gráfico
fig2
```

```{r}
library(plotly)

# Crear gráfico 3D
fig3 <- plot_ly(
  data = result_data,
  x = ~minsplit,
  y = ~minbucket,
  z = ~Value,
  type = 'scatter3d',
  mode = 'markers',
  marker = list(size = 5, color = ~Value, colorscale = 'Viridis', showscale = TRUE)
)

# Configurar el título y los ejes
fig3 <- fig3 %>%
  layout(
    title = 'Progreso del AUC en función de maxdepth y minbucket',
    scene = list(
      xaxis = list(title = 'minsplit'),
      yaxis = list(title = 'minbucket'),
      zaxis = list(title = 'Área Bajo la Curva (AUC)')
    )
  )

# Mostrar el gráfico
fig3
```


```{r}

# Define the grid search limits
grid_search_limites <- list(
  maxdepth = c(25:30), 
  minsplit = c(85:95), 
  minbucket = c(8:10)
)

# Create the grid search
grid_search <- expand.grid(grid_search_limites)

# Placeholder for results
results_gs <- data.frame(
  max_depth = integer(),
  min_split = integer(),
  min_bucket = integer(),
  performance = numeric()
)
# Loop through each combination of parameters
for (i in 1:nrow(grid_search)) {
  model_gd <- rpart(
    popularity ~ ., 
    data = train_set, 
    method = "class", 
    control = rpart.control(
      maxdepth = grid_search$maxdepth[i],
      minsplit = grid_search$minsplit[i],
      minbucket = grid_search$minbucket[i],
      cp = 0,
      xval = 0
    )
  )
  
  performance <- evaluar_arbol(grid_search$maxdepth[i], 
                               grid_search$minsplit[i],
                               grid_search$minbucket[i],
                               train_set,
                               val_set)
  
  # Store the results in the data frame
  results_gs <- rbind(results_gs, data.frame(
    max_depth = grid_search$maxdepth[i],
    min_split = grid_search$minsplit[i],
    min_bucket = grid_search$minbucket[i],
    performance = performance
  ))
  
  #printcp(model)  # Print complexity parameter table to check tree complexity
  #plotcp(model)   # Visualize the complexity of the decision tree
  
  # Debugging prints
  print(paste("maxdepth =", grid_search$maxdepth[i], 
              "minsplit =", grid_search$minsplit[i], 
              "minbucket =", grid_search$minbucket[i], 
              "Performance =", performance))
}

# View the results
print(results_gs)
max(results_gs$Score)

```

```{r}
# Cargar las librerías necesarias
library(rpart)
library(rpart.plot)

# Crear el modelo utilizando los hiperparámetros especificados
mejor_arbol_gd <- rpart(
  formula = popularity ~ ., 
  data = train_set, 
  method = "class", 
  control = rpart.control(
    maxdepth = 27, 
    minsplit = 97, 
    minbucket = 10,
    cp = 0,
    xval = 0
  )
)

# Graficar el árbol utilizando rpart.plot
rpart.plot(mejor_arbol_gd, box.palette = "violet")
print(mejor_arbol_gd$control)
```

 -> Elegir el mejor árbol y medir el valor de AUC-ROC con el conjunto de testeo
 
```{r}
# Cargar las librerías necesarias
library(rpart)
library(rpart.plot)

# Crear el modelo utilizando los hiperparámetros especificados
mejor_arbol <- rpart(
  formula = popularity ~ ., 
  data = train_set, 
  method = "class", 
  control = rpart.control(
    maxdepth = 28, #15
    minsplit = 98, #27
    minbucket = 10, #2
    cp = 0,
    xval = 0
  )
)

# Graficar el árbol utilizando rpart.plot
rpart.plot(mejor_arbol, box.palette = "orange")
print(mejor_arbol$control)
```
 
 -> Comparar el rendimiento entre el primer árbol y el árbol optimizado
 
```{r}
predicciones_modelo = predict(mejor_arbol, newdata = test_set, type = "prob")[,2]

test_set$predicciones_optimas = predicciones_modelo
test_set$predicciones_optimas <- ifelse(test_set$predicciones_optimas >= 0.5, 1, 0)
test_set$predicciones_optimas <- as.factor(test_set$predicciones_optimas)

curva_optimizada <- roc(test_set$popularity, predicciones_modelo)
auc_score_mejor_arbol <- auc(curva_optimizada)
auc_score_mejor_arbol

# Graficar la primera curva
plot(curva_default, col = "orange", lwd = 3, main = "Curvas ROC", xlab = "Tasa de Falsos Positivos", ylab = "Tasa de Verdaderos Positivos")

# Añadir la segunda curva al mismo gráfico
lines(curva_optimizada, col = "purple", lwd = 3)

# Añadir una leyenda para diferenciar las curvas
legend("bottomright", legend = c("Curva árbol básico", "Curva árbol optimizado"), col = c("orange", "purple"), lwd = 3)
```

```{r}
predicciones_modelo_gd = predict(mejor_arbol_gd, newdata = test_set, type = "prob")[,2]

test_set$predicciones_optimas_gd = predicciones_modelo_gd
test_set$predicciones_optimas_gd <- ifelse(test_set$predicciones_optimas_gd >= 0.5, 1, 0)
test_set$predicciones_optimas_gd <- as.factor(test_set$predicciones_optimas_gd)

curva_optimizada_gd <- roc(test_set$popularity, predicciones_modelo_gd)

auc_score_mejor_arbol_gd <- auc(curva_optimizada_gd)
auc_score_mejor_arbol_gd
```


```{r}
#comparacion matrices de correlacion

graficar_matriz_confusion(cm)

cm_optimizada = generar_matriz_confusion(test_set$popularity, test_set$predicciones_optimas)
graficar_matriz_confusion(cm_optimizada)
```


 6. Interpretación de resultados
 
 -> Presentar el árbol final, y detallar las diferencias entre el primero y el último
 
```{r}
library(corrplot)

muestra_numerica <- muestra
muestra_numerica$popularity <- as.numeric(as.character(muestra_numerica$popularity))

  

numeric_data <- muestra_numerica[, c('popularity', 'duration_min', 'explicit', 'danceability', 
                                  'energy', 'key', 'loudness', 'mode', 
                                  'speechiness', 'acousticness', 
                                  'instrumentalness', 'liveness', 
                                  'valence', 'tempo', 'time_signature')]


# Calcular la matriz de correlación
cor_matrix <- cor(numeric_data, use = "complete.obs")

# Mostrar la matriz de correlación
print(cor_matrix)

color_pal <- colorRampPalette(c("#081D59", "white", "#FD0604"))(200)

# Visualizar la matriz de correlación usando corrplot
corrplot(cor_matrix, method = "color", type = "upper", tl.col = "black", 
         tl.cex = 0.8, title = "Matriz de Correlación", mar = c(0,0,1,0), col = color_pal)

```

 -> Identificar y discutir las variables más importantes según nuestro mejor árbol
 
 [ACA]
 
 7. Análisis del impacto de los valores faltantes
 
 -> Tenemos que generar 3 datasets, en cada uno falta un % de los datos de las predictoras.     En el primero 20%, el segundo 50% y el tercero 75%
 
```{r}
# Función para reemplazar un porcentaje de valores al azar con NA
reemplazar_con_na <- function(dataset, porcentaje) {
  datos_na <- dataset
  for (columna in names(datos_na)) {
    if (is.numeric(datos_na[[columna]]))  {
      cantidad_na <- round(porcentaje * nrow(datos_na))
      indices <- sample(1:nrow(datos_na), cantidad_na)
      datos_na[indices, columna] <- NA
    }
  }
  return(datos_na)
}

# Creación de los tres sets
train_20_na <- reemplazar_con_na(train_set, 0.20)
val_20_na <- reemplazar_con_na(val_set, 0.20)
test_20_na <- reemplazar_con_na(test_set, 0.20)

train_50_na <- reemplazar_con_na(train_set, 0.50)
val_50_na <- reemplazar_con_na(val_set, 0.50)
test_50_na <- reemplazar_con_na(test_set, 0.50)

train_75_na <- reemplazar_con_na(train_set, 0.75)
val_75_na <- reemplazar_con_na(val_set, 0.75)
test_75_na <- reemplazar_con_na(test_set, 0.75)
```
 
 -> Para cada dataset creamos un nuevo árbol optimizado (elegir hiperparam. que maximicen)
    Comparar el mejor árbol obtenido antes, con los 3 nuevos árboles
    

```{r}

# Ejecutar la optimización bayesiana
result_20_NA <- BayesianOptimization(
  FUN = function(maxdepth, minsplit, minbucket) {
    evaluar_arbol(maxdepth, minsplit, minbucket, train_20_na, val_20_na)
  },
  bounds = limites,
  init_points = 5,
  n_iter = 10,
)
result_data_20_NA <- result_20_NA$History

graficar_performance(result_data_20_NA)

```
```{r}
# Crear el modelo utilizando los hiperparámetros especificados
mejor_arbol_20_na <- rpart(
  formula = popularity ~ ., 
  data = train_20_na, 
  method = "class", 
  control = rpart.control(
    maxdepth = 17, 
    minsplit = 71, 
    minbucket = 7,
    cp = 0,
    xval = 0
  )
)

# Graficar el árbol utilizando rpart.plot
rpart.plot(mejor_arbol_20_na, box.palette = "violet")
print(mejor_arbol_20_na$control)
```

```{r}
predicciones_modelo_20_na = predict(mejor_arbol_20_na, newdata = test_20_na, type = "prob")[,2]

test_20_na$predicciones_optimas_20_na = predicciones_modelo_20_na
test_20_na$predicciones_optimas_20_na <- ifelse(test_20_na$predicciones_optimas_20_na >= 0.5, 1, 0)
test_20_na$predicciones_optimas_20_na <- as.factor(test_20_na$predicciones_optimas_20_na)

curva_optimizada_20_na <- roc(test_20_na$popularity, predicciones_modelo_20_na)

auc_score_mejor_arbol_20_na <- auc(curva_optimizada_20_na)
auc_score_mejor_arbol_20_na
```


```{r}
# Ejecutar la optimización bayesiana
result_50_NA <- BayesianOptimization(
  FUN = function(maxdepth, minsplit, minbucket) {
    evaluar_arbol(maxdepth, minsplit, minbucket, train_50_na, val_50_na)
  },
  bounds = limites,
  init_points = 5,
  n_iter = 10,
)

result_data_50_NA <- result_50_NA$History

graficar_performance(result_data_50_NA)

```


```{r}
# Ejecutar la optimización bayesiana
result_75_NA <- BayesianOptimization(
  FUN = function(maxdepth, minsplit, minbucket) {
    evaluar_arbol(maxdepth, minsplit, minbucket, train_75_na, val_75_na)
  },
  bounds = limites,
  init_points = 5,
  n_iter = 10,
)

result_data_75_NA <- result_75_NA$History

graficar_performance(result_data_75_NA)

```

    
  -> Analizar como cambia el rendimiento del modelo a medida que aumenta la cantidad de NAs (Recomiendan fuertemente usar gráficos)
  
  [ACA]

8. Conclusiones y discusión

  -> Resumir los hallazgos principales del análisis.
  
  [ACA]
  
  -> Reflexionar sobre la efectividad del árbol de decisión para el problema planteado.
  
  [ACA]
  
  -> Sugerir posibles mejoras o direcciones futuras para el análisis.
  
  [ACA]


